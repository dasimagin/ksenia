# Ksenia (Ksenia stores externally now it all)

### Oleges plans
* NTM version with RL:
   - [ ] Discrete attention schemes from [D-NTM](https://github.com/caglar/dntm)
   - [ ] Combine dicre—Årete and traditional learning like in mentioned above paper
   - [ ] Q-Learning and (maybe) Watkins Q-dynamic from [Learning simple algorithms from examples](https://github.com/wojzaremba/algorithm-learning), paper can be [found](https://arxiv.org/abs/1511.07275)
   - [ ] Penalty on Q-function (from same paper)
   - [ ] Compare on different datasets

* Update NTM with additional critic based on different models:
   - [ ] Understand Recurrent Deterministic Policy Gradient from [Memory-based control with recurrent neural networks](https://paperswithcode.com/paper/memory-based-control-with-recurrent-neural)
   - [ ] Try convolutional critic based on memory
   - [ ] Try different learning schemes:
     - [ ] Q-learning
     - [ ] Watkins Q-dynamic
     - [ ] Penalty on Q-function
   - [ ] Compare on different datasets

* Implement some more experimets:
  - [ ] Convex Hill of set of point [link](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.ConvexHull.html)
* Read papers:
  - [ ] [Memory-based control with recurrent neural networks](https://paperswithcode.com/paper/memory-based-control-with-recurrent-neural)
  - [ ] [GLOBAL-TO-LOCAL MEMORY POINTER NETWORKS FOR TASK-ORIENTED DIALOGUE](https://arxiv.org/pdf/1901.04713v1.pdf)
  - [ ] [Learning to Remember More with Less Memorization](https://arxiv.org/pdf/1901.01347.pdf)
